---
title:  "Avito BI contest"
excerpt: "Разбор одной задачи из Avito BI contest"
date:   2017-02-10T00:00:00+03:00
categories:
   - Snort
tags:
   - Python
   - Scrapy
   - Web scraping
   - Parsing
author_profile: true
---

{% include toc icon="file-text" %}

В данной статье я хотел бы рассказать о своём решении первой задачи из [Avito BI contest](https://boosters.pro/champ_4), в которой необходимо было парсить данные с сайта Avito. Формальный текст задачи:

>Задача 1.
Ответом в первой задаче будут два числа. Вам предстоит распарсить объявления и А) узнать сколько на сайте Avito в городе Москве объявлений в категории "Монеты" (Хобби и отдых -> Коллекционирование -> Монеты) без указания срока выпуска монеты, а также Б) какое соотношение монет выпущенных до 2000 года, к монетам выпущенным после.
Допущения, которыми можно руководствоваться при решении задачи:
- Если в одном объявлении продают сразу несколько монет, то нужно брать год выпуска первого предложения.
- Если указан не точный год, а интервал выпуска монеты, то нужно брать нижнюю границу интервала
При парсинге объявлений рекомендуется использовать Python.
Ответ меняется с течением времени, поэтому отправлять нужно последнее (заменит слово) решение.
Число отправок решения – 2 на время всего чемпионата.


Для решения задачи я использовал язык программирования Python, как и предлагалось в задаче, а также фреймворк для парсинга сайтов [Scrapy](https://scrapy.org). Разобраться в данном фреймворке можно как по книге [Dimitrios Kouzis-Loukas, "Learning Scrapy"](http://scrapybook.com/), так и прочитав его документацию [Scrapy 1.3 documentation](https://doc.scrapy.org/en/latest/).

Для начала открывает нужный нам раздел на Avito для парсинга https://www.avito.ru/moskva/kollektsionirovanie/monety. В нём на данный момент находится примерно 35 000 объявлений о продаже монет в Москве. На каждой странице по 53 объявления о продаже, таким образом примерно 660 страниц по 53 объявления. В процессе парсинга я обнаружил, что Avito отдаёт только 100 страниц. То есть можно открыть 100-ую страницу со списком объявлений о продаже монет https://www.avito.ru/moskva/kollektsionirovanie/monety?p=100, а при попытке перейти на 101-ую https://www.avito.ru/moskva/kollektsionirovanie/monety?p=101 происходит редирект на первую страницу. А нам ведь требуется собрать ссылки на каждое объявление, чтобы уже после анализировать их содержание. Обойти данное ограничение в 100 страниц я решил разделив скрапинг по станциям метро. Вместо одного поиска по всем объявлениям в Москве я решил сделать N-поисков по количеству станций метро в Москве (на данный момент их 185), предположив что на одной станции метро не будет большее 53*100 объявлений. Строка поиска, уточняющая станцию метро, будет иметь вид `https://www.avito.ru/moskva/kollektsionirovanie/monety?metro=N`, где N - число, идентификатор станции метро. С помощью инструмента браузера **Inspect Element** (Ctrl+Shift+I) можно посмотреть соответствия названий станций метро и их идентификаторов на Avito. Выглядит это следующим образом:

```html
<select id="directions-select" class="directions" data-filter="1"> <option data-prev-alias="metro" value="">Станция метро</option>
  <option value="1">Авиамоторная</option>
  <option value="2">Автозаводская</option>
  <option value="3">Академическая</option>
  <option value="4">Александровский сад</option>
  <option value="151">Алексеевская</option>
  <option value="2135">Алма-Атинская</option>
  <option value="5">Алтуфьево</option>
  <option value="148">Аннино</option>

  ...
  ...
  ...

  <option value="140">Шаболовская</option>
  <option value="216">Шипиловская</option>
  <option value="141">Шоссе Энтузиастов</option>
  <option value="142">Щелковская</option>
  <option value="143">Щукинская</option>
  <option value="144">Электрозаводская</option>
  <option value="145">Юго-Западная</option>
  <option value="146">Южная</option>
  <option value="147">Ясенево</option>
  </select>
```

Извлечь данные номера достаточно просто с использованием регулярных выражений. Например, скопировав вышеприведённые список станций метро в текстовый редактор Atom и заменив все совпадения `^\s+?<option value="(\d+?)">[^<]+?</option>` на `$1, `. Если думать в терминах Scrapy, то начальные ссылки (start_urls) для каждой станции метро можно задать с помощью генератора списков:

```python
stations_numbers = [1, 2, 3, 4, 151, 2135, 5, 148, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 2145, 215, 18, 19, 20, 1010,
                    149, 127, 1012, 2155, 22, 21, 23, 24, 25, 26, 27, 1003, 28, 152, 29, 2146, 30, 31, 32, 33, 2001, 34,
                    2143, 217, 35, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 2151, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56,
                    57, 58, 59, 2142, 2144, 60, 61, 62, 2002, 63, 64, 65, 1004, 66, 1001, 67, 1002, 68, 69, 70, 71,
                    2133, 72, 73, 17, 74, 75, 76, 77, 78, 79, 80, 82, 81, 36, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92,
                    93, 94, 95, 96, 97, 98, 99, 2136, 100, 101, 102, 2149, 103, 104, 2150, 105, 106, 107, 108, 1005,
                    109, 110, 111, 2147, 112, 1007, 214, 113, 114, 115, 116, 117, 118, 119, 120, 2152, 121, 122, 2148,
                    1006, 123, 124, 125, 126, 128, 1011, 1009, 1008, 129, 130, 131, 2154, 132, 133, 134, 135, 136, 137,
                    138, 139, 140, 216, 141, 142, 143, 144, 145, 146, 147]
start_urls = ['https://m.avito.ru/moskva/kollektsionirovanie/monety?metro=' + str(station_number) for station_number
              in stations_numbers]
```

Вначале я не обратил внимание, что список всех станций присутствует в html-странице и их оттуда можно извлечь, и начал сканировать предполагаемые номера станций метро следующим образом (но Avito достаточно быстро блокирует доступ к своему ресурсу по IP после такого агрессивного сканирования):

```python
import requests
print([base_url + str(station_number) for station_number in range(1, 10000) if
       requests.get(base_url + str(station_number),
                    headers={'User-Agent': 'Mozilla/5.0'},
                    allow_redirects=False).status_code == 200])
```

Если с начальными ссылками (start_urls) мы определились, то теперь нам нужно задать для Scrapy правила для так называемых вертикального и горизонтального движения. Вертикальное перебирает все объявление на одной странице, а горизонтальное перебирает номера страниц.
